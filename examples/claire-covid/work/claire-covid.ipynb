{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAIRE-COVID19 universal pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLAIRE-COVID19 universal pipeline has been designed to compare different training algorithms for the AI-assisted diagnosis of COVID-19, in order to define a baseline for such techniques and to allow the community to quantitatively measure AIâ€™s progress in this field. For more information, please read [this post](https://streamflow.di.unito.it/2021/04/12/ai-assisted-covid-19-diagnosis-with-the-claire-universal-pipeline/).\n",
    "\n",
    "This notebook showcases how the classification-related portion of the pipeline can be successfully described and documented in Jupyter, using the *Jupyter-workflow* to execute it at scale on an HPC facility. \n",
    "\n",
    "The first preprocessing portion of the pipeline is left outside the notebook. It could clearly (and effectively) be included, but since this example wants to serve as an introductory demonstration, we wanted to keep it as simple as possible.\n",
    "\n",
    "Let's start with a simple import of all the required Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.hub\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nnframework.data_loader as module_dataloader\n",
    "import nnframework.graphs.models as module_model\n",
    "import nnframework.graphs.losses as module_loss\n",
    "import nnframework.graphs.metrics as module_metric\n",
    "\n",
    "from metrics.results import Results\n",
    "\n",
    "from nnframework.parse_config import ConfigParser\n",
    "from nnframework.trainers import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to setup the experiment's parameter. As an example, we decided to run a grid search on a pre-trained `DenseNet-121`, fine-tuning it on a pre-processed and filtered version of the [BIMCV-COVID19+](https://bimcv.cipf.es/bimcv-projects/bimcv-covid19/) dataset (Iteration 1) with different combinations of Learning Rate (LR), LR decay step and weight decay. Moreover, in order to enhance the robustness of the accuracy metrics, we also perform a 5-fold cross-validation for each of the training configurations.\n",
    "\n",
    "Since this is only a portion of the CLAIRE-COVID19 universal pipeline, there are some manual preliminary steps to be done in order to successfully run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, you need to produce a pre-processed and filtered version of the BIMCV-COVID19+ dataset (Iteration 1). Instructions on how to do this can be found [here](https://github.com/CLAIRE-COVID/AI-Covid19-preprocessing) and [here](https://github.com/CLAIRE-COVID/AI-Covid19-pipelines).\n",
    "\n",
    "This procedure will produce three different versions of the dataset. For this experiment, we need the `final3_BB` folder, which can be either manually pre-transferred on the remote HPC facility (as we did) or left on the local machine, letting *Jupyter-workflow* automatically perform the data transfer when needed.\n",
    "\n",
    "In the first case, you simply have to insert the correct value of the `dataset_path` variable, while in the second case you have to put your local dataset path in the `dataset_path` variable and then explicitly including it in the remote step dependencies as\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"file\",\n",
    "  \"valueFrom\": \"dataset_path\"\n",
    "}\n",
    "```\n",
    "\n",
    "What discussed so far for the `dataset_path` applies also to the `labels_path`, which must contain the path to the `labels_covid19_posi.tsv` file produced in output by the preprocessing portion of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, if you plan to run on an air-gapped architecture (as HPC facilities normally are), you will need to manually download all the pre-trained neural network models and place them into a `weights` folder, in a portion of file-system shared among all the worker nodes in the data centre. This can be done by running [this script](https://raw.githubusercontent.com/CLAIRE-COVID/AI-Covid19-benchmarking/master/nnframework/init_models.py). We could also include this script as a notebook cell, but again we didn't to keep the Notebook as linear as possible.\n",
    "\n",
    "Again, you have two different ways to manage this task. You can either run the task directly on the remote machine, and put the path of the `weights` folder in the `weights_path` variable. Or, you can run the script locally and let *Jupyter-workflow* automatically manage the data transfer for you, explicitly defining the `weights_path` as a file variable as discussed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to transfer dependencies (i.e., PyTorch and the CLAIRE-COVID19 benchmarking [code](https://github.com/CLAIRE-COVID/AI-Covid19-benchmarking)) on the remote facility. To enhance portability, we used a Singularity container with everything inside, but you have to manually transfer it on the remote HPC facility, in a shared portion of the file-system, and to change the `environment/cineca-marconi100/slurm_template.jinja2` file accordingly. We plan to make this last transfer automatically managed by *Jupyter-workflow* in the very next releases.\n",
    "\n",
    "If you plan to run on an x86_64 architecture, creating the Singularity is as easy as trandforming the Docker image for this experiment in a Singularity image. Neverhteless, the [MARCONI 100](https://www.hpc.cineca.it/hardware/marconi100) HPC facility comes with an IBM POWER9 architecture. Therefore, we built a Singularity container on a `ppc64le` architecture using [this script](https://raw.githubusercontent.com/alpha-unito/jupyter-workflow/master/examples/claire-covid/singularity/ppcle64/build)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = '/path/to/final3_BB'\n",
    "epochs = 1\n",
    "gpus = 1\n",
    "k_folds = list(range(5))\n",
    "labels_path = '/path/to/labels_covid19_posi.tsv'\n",
    "learning_rates = [0.001, 0.0001, 0.00001]\n",
    "lr_step_sizes = [10, 15]\n",
    "model_versions = [121]\n",
    "model_type = 'DenseNetModel'\n",
    "weight_decays = [0.0005, 0.00005]\n",
    "weights_path = '/path/to/models/weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the *Jupyter-workflow* `scatter` option to iterate over the cartesian product of experiment configurations, producing a config file for eath training instance.\n",
    "\n",
    "Please not that we perform this step locally, prior to transferring the computation to the remote HPC facility. We can easily separate these two steps as we do not need to move the dataset and the model weights, and therefore we already know their remote paths.\n",
    "\n",
    "Conversely, it would be probably easier to merge this code cell with the following, producing the configuration when data has been already transferred. Alternatively, it would be possible to rely on the `predump` and `postload` features to modify the value of the `config` variable on the remote node, but this would be much harder than simply merging two code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "workflow": {
     "interpreter": "/opt/conda/bin/ipython",
     "step": {
      "in": [
       {
        "name": "k_folds",
        "scatter": {},
        "type": "name"
       },
       {
        "name": "learning_rates",
        "scatter": {},
        "type": "name"
       },
       {
        "name": "lr_step_sizes",
        "scatter": {},
        "type": "name"
       },
       {
        "name": "model_versions",
        "scatter": {},
        "type": "name"
       },
       {
        "name": "weight_decays",
        "scatter": {},
        "type": "name"
       }
      ],
      "out": [
       "configs"
      ]
     }
    }
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "for model_version in model_versions:\n",
    "    for learning_rate in learning_rates:\n",
    "        for lr_step_size in lr_step_sizes:\n",
    "            for weight_decay in weight_decays:\n",
    "                for k_fold_idx in k_folds:\n",
    "                    name = f'{model_type}{model_version}_lr{learning_rate}_step{lr_step_size}_wd{weight_decay}'.format(\n",
    "                        model_type=model_type,\n",
    "                        model_version=model_version,\n",
    "                        learning_rate=learning_rate,\n",
    "                        lr_step_size=lr_step_size,\n",
    "                        weight_decay=weight_decay)\n",
    "                    output_folder = 'training_{name}'.format(name=name)\n",
    "                    configs.append({\n",
    "                        'name': name,\n",
    "                        'n_gpu': gpus,\n",
    "                        'weights_path': weights_path,\n",
    "                        'arch': {\n",
    "                            'type': model_type,\n",
    "                            'args': {\n",
    "                                'variant': model_version,\n",
    "                                'num_classes': 2,\n",
    "                                'print_model': True\n",
    "                            }\n",
    "                        },\n",
    "                        'loss': 'cross_entropy_loss',\n",
    "                        'metrics': [\n",
    "                            'accuracy'\n",
    "                        ],\n",
    "                        'data_loader': {\n",
    "                            'type': 'COVID_Dataset',\n",
    "                            'args': {\n",
    "                                'root': dataset_path,\n",
    "                                'k_fold_idx': k_fold_idx,\n",
    "                                'mode': 'ct',\n",
    "                                'pos_neg_file': labels_path,\n",
    "                                'splits': [0.7, 0.15, 0.15],\n",
    "                                'replicate_channel': 1,\n",
    "                                'batch_size': 64,\n",
    "                                'input_size': 224,\n",
    "                                'num_workers': 2,\n",
    "                                'self_supervised': 0\n",
    "                            }\n",
    "                        },\n",
    "                        'optimizer': {\n",
    "                            'type': 'Adam',\n",
    "                            'args': {\n",
    "                                'lr': learning_rate,\n",
    "                                'weight_decay': weight_decay,\n",
    "                                'amsgrad': True\n",
    "                            }\n",
    "                        },\n",
    "                        'lr_scheduler': {\n",
    "                            'type': 'StepLR',\n",
    "                            'args': {\n",
    "                                'step_size': lr_step_size,\n",
    "                                'gamma': 0.1\n",
    "                            }\n",
    "                        },\n",
    "                        'trainer': {\n",
    "                            'epochs': epochs,\n",
    "                            'save_dir': output_folder,\n",
    "                            'save_period': 1,\n",
    "                            'verbosity': 2,\n",
    "                            'monitor': 'min val_loss',\n",
    "                            'early_stop': 10,\n",
    "                            'tensorboard': False\n",
    "                        }\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform the main training cycle on the CINECA [MARCONI 100](https://www.hpc.cineca.it/hardware/marconi100) HPC facility. Details about the execution are stored in the `environment/cineca-marconi100/slurm_template.jinja2` file.\n",
    "\n",
    "*Jupyter-workflow* will automatically transfer all the required dependencies on the login node of the HPC facility prior to run the `sbatch` command with the selected template. Nevertheless, it will not perform any automatic transfer from the login node to the worker nodes. Therefore, please configure the `workdir` directive in the cell metadata to point to a shared potion of the file-system, where both the login node and the worker nodes of the data center are allowed to access.\n",
    "\n",
    "In order to repeat the experiment, you have to change the `username` and `sshKey` fields in the cell metadata with your credentials. Moreover, to move the computation on a different Slurm-managed environment, you will also have to change the `hostname` field and the contents of the `slurm_template.jinja2` file.\n",
    "\n",
    "Alternatively, *Jupyter-workflow* also supports PBS-based HPC facilities in an analogous manner. Take a look at the [quantum-espresso example](https://github.com/alpha-unito/jupyter-workflow/tree/master/examples/quantum-espresso) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "workflow": {
     "interpreter": "/opt/conda/bin/python",
     "step": {
      "in": [
       {
        "name": "configs",
        "scatter": {},
        "type": "name"
       }
      ],
      "out": [
       {
        "name": "results_dirs",
        "type": "file",
        "valueFrom": "results_dirs"
       }
      ],
      "workdir": "/path/to/shared/workdir"
     },
     "target": {
      "model": "cineca-marconi100"
     }
    }
   },
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "results_dirs = []\n",
    "for config in configs:\n",
    "    config = ConfigParser(config)\n",
    "    results_dirs.append(config.results_dir)\n",
    "    \n",
    "    logger = config.get_logger('train')\n",
    "    data_loader = config.init_obj('data_loader', module_dataloader)\n",
    "    \n",
    "    torch.hub.set_dir(config['weights_path'])\n",
    "    model = config.init_obj('arch', module_model)\n",
    "    logger.info(model)\n",
    "    \n",
    "    if config['data_loader']['args']['self_supervised']:\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss(weight=data_loader.get_label_proportions().to('cuda'))\n",
    "        \n",
    "    metrics = [getattr(module_metric, met) for met in config['metrics']]\n",
    "    \n",
    "    trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n",
    "    lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)\n",
    "    \n",
    "    trainer = Trainer(model.get_model(), criterion, metrics, optimizer,\n",
    "                      config=config,\n",
    "                      train_data_loader=data_loader.train,\n",
    "                      valid_data_loader=data_loader.val,\n",
    "                      test_data_loader=data_loader.test,\n",
    "                      lr_scheduler=lr_scheduler)\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the metrics results produced by each training step are stored in a local folder, as *Jupyter-workflow* automatically transferred them from the remote file-system. The implicit `gather` strategy of the previous cell merged all the paths of such folders in a list variable, named `results_dirs`. This list can be now processed to, for example, compute more sophisticated metrics and visualising them interactively on the Jupyter `matplotlib` backend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(math.ceil(len(results_dirs)/2), 2)\n",
    "fig.suptitle('ROC curves')\n",
    "for i, results_dir in enumerate(results_dirs):\n",
    "    res = Results(path=results_dir)\n",
    "    res.load()\n",
    "    \n",
    "    for p in tqdm(range(0,102)):\n",
    "        res.compute_metrics_per_scan_proportion(proportion=p/100)\n",
    "    res.save(overwrite=True)\n",
    "    \n",
    "    data = res.results_per_scan_proportion\n",
    "    sens = []\n",
    "    spec = []\n",
    "    for k in sorted(data.keys()):\n",
    "        sens.append(res.results_per_scan_proportion[k]['sensitivity'])\n",
    "        spec.append(res.results_per_scan_proportion[k]['specificity'])\n",
    "    sens = np.array(sens)\n",
    "    spec = np.array(spec)\n",
    "    axes[i // 2, i % 2].title.set_text(os.path.basename(results_dir)[12:-6])\n",
    "    axes[i // 2, i % 2].set(xlabel='FPR', ylabel='TPR')\n",
    "    axes[i // 2, i % 2].plot(1-spec, sens)\n",
    "if math.ceil(len(results_dirs)/2) % 2 != 0:\n",
    "    fig.delaxes(axes[len(results_dirs) // 2, 1])\n",
    "plt.tight_layout()\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "workflow": {
   "models": {
    "cineca-marconi100": {
     "config": {
      "file": "environment/cineca-marconi100/slurm_template.jinja2",
      "hostname": "login.m100.cineca.it",
      "maxConcurrentJobs": 60,
      "sshKey": "/path/to/ssh/key",
      "username": "username"
     },
     "type": "slurm"
    }
   },
   "version": "v1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}