{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and serve a TensorFlow model with TensorFlow Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook showcases how Deep Learning can benefit from hybrid Cloud-HPC scenarios, in particular to offload the computationally demanding training phase to an HPC facility, and offering the resulting network as-a-Service on a Cloud achitecture to answer time-constrained inference queries.\n",
    "\n",
    "The notebook is a modified version of [this one](https://github.com/tensorflow/tfx/blob/master/docs/tutorials/serving/rest_simple.ipynb), which focuses mainly on Google Colab. Since we are not interested in pure performance for this use case, we use the small [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and a very small custom Convolutional Neural Network (CNN).\n",
    "\n",
    "The training phase has been offloaded to an HPC node on the [HPC4AI facility](https://hpc4ai.unito.it/), equiped with four NVIDIA Tesla V100 and two Intel Xeon Gols 6230 sockets. Then, the trained model is sent to a pre-existing TensorFlow Serving Pod on a Kubernetes cluster, hosted on the OpenStack-based  HPC4AI cloud.\n",
    "\n",
    "Let's start with an import of all the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major == 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n",
    "\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "print('TensorFlow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the Fashion MNIST dataset, containing 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n",
    "\n",
    "<table>\n",
    "  <tr><td>\n",
    "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
    "         alt=\"Fashion MNIST sprite\"  width=\"600\">\n",
    "  </td></tr>\n",
    "  <tr><td align=\"center\">\n",
    "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
    "  </td></tr>\n",
    "</table>\n",
    "\n",
    "Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) datasetâ€”often used as the \"Hello, World\" of machine learning programs for computer vision. You can access the Fashion MNIST directly from TensorFlow, just import and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 1s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n"
     ]
    }
   ],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "# scale the values to 0.0 to 1.0\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# reshape for feeding into the model\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we create our neural network model. Since we are not focused on performances, let's use the simplest possible CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3, \n",
    "                      strides=2, activation='relu', name='Conv1'),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model for one epoch. Since this is commonly a computationally demanding step, we offload it to an HPC node through an SSH connector. For portability purposes, we execute the command on a Docker container on the remote environment, created from a standard `jupyter/tensorflow-notebook` image.\n",
    "\n",
    "In order to replicate the experiment, you need to set the correct credentials in the Notebook metadata (e.g., `username` and `sshKey`) and, if necessary, modify the script template at `environment/hpc/ssh_template.jinja2`.\n",
    "\n",
    "Please note that the TensorFlow network was not trivially serialisable by the `dill` engine, so we were forced to implement custom `predump` and `postload` directives to correctly handle the data transfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "workflow": {
     "step": {
      "in": [
       {
        "name": "model",
        "serializer": "model",
        "type": "name"
       }
      ],
      "out": [
       {
        "name": "model",
        "serializer": "model",
        "type": "name"
       }
      ]
     },
     "target": {
      "model": "tensorflow"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxicla_tz/assets\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 4 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "1875/1875 [==============================] - 44s 8ms/step - loss: 0.7282 - accuracy: 0.7529\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4243 - accuracy: 0.8494\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3735 - accuracy: 0.8649\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3539 - accuracy: 0.8724\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.3264 - accuracy: 0.8826\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3186 - accuracy: 0.8842\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3028 - accuracy: 0.8911\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2972 - accuracy: 0.8943\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2881 - accuracy: 0.8956\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2820 - accuracy: 0.8975\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2776 - accuracy: 0.9008\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2719 - accuracy: 0.9026\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2635 - accuracy: 0.9045\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2643 - accuracy: 0.9057\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2586 - accuracy: 0.9065\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2586 - accuracy: 0.9065\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2517 - accuracy: 0.9082\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2481 - accuracy: 0.9095\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2485 - accuracy: 0.9095\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2477 - accuracy: 0.9113\n",
      "313/313 [==============================] - 16s 46ms/step - loss: 0.3317 - accuracy: 0.8833\n",
      "\n",
      "Test accuracy: 0.8833000063896179\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpvq_9klt9/assets\n"
     ]
    }
   ],
   "source": [
    "testing = False\n",
    "epochs = 20\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = tf.keras.models.clone_model(model)\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_images, train_labels, epochs=epochs)\n",
    "    test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "    \n",
    "print('\\nTest accuracy: {}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, the `model` variable contains the trained model's weights. To load our trained model into TensorFlow Serving we first need to save it in [SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model) format.  This will create a protobuf file in a well-defined directory hierarchy, and will include a version number.\n",
    "\n",
    "The path of the saved model is stored in the `export_path` variable to be easily referred to in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export_path = /tmp/tmpeji93ti1\n",
      "\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpeji93ti1/assets\n",
      "\n",
      "Saved model:\n",
      "total 72\n",
      "drwxr-xr-x 2 jovyan users  4096 Apr 15 23:44 assets\n",
      "-rw-r--r-- 1 jovyan users 63956 Apr 15 23:44 saved_model.pb\n",
      "drwxr-xr-x 2 jovyan users  4096 Apr 15 23:44 variables\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "export_path = tempfile.mkdtemp()\n",
    "print('export_path = {}\\n'.format(export_path))\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "    model,\n",
    "    export_path,\n",
    "    overwrite=True,\n",
    "    include_optimizer=True,\n",
    "    save_format=None,\n",
    "    signatures=None,\n",
    "    options=None\n",
    ")\n",
    "\n",
    "print('\\nSaved model:')\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can send our model to a [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) instance for inference. We deploy a Kubernetes Pod containing a `tensorflow-serving` container, send the model folder as an input dependency and copy it on the `/models/model` path in the remote file-system, in order to start serving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "workflow": {
     "step": {
      "in": [
       {
        "type": "file",
        "valueFrom": "export_path"
       }
      ]
     },
     "target": {
      "model": "tf-serving",
      "service": "tensorflow-serving"
     }
    }
   },
   "outputs": [],
   "source": [
    "%%bash -s {export_path}\n",
    "rm -rf \"/models/model\"\n",
    "mkdir \"/models/model\"\n",
    "cp -r \"$1\" \"/models/model/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test if the inference server to test is actually able to correctly classify objects. Since the service is not publicly exposed, we use *Jupyter-workflow* to connect to the remote service and perform a local HTTP request to obtain the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "workflow": {
     "step": {
      "out": [
       "predictions"
      ]
     },
     "target": {
      "model": "tf-serving",
      "service": "tensorflow-serving"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: {\"signature_name\": \"serving_default\", \"instances\": ...  [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]]]]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "data = json.dumps({\"signature_name\": \"serving_default\", \"instances\": test_images[0:3].tolist()})\n",
    "print('Data: {} ... {}'.format(data[:50], data[len(data)-52:]))\n",
    "\n",
    "headers = {\"content-type\": \"application/json\"}\n",
    "json_response = requests.post('http://localhost:8501/v1/models/model:predict', data=data, headers=headers)\n",
    "predictions = json.loads(json_response.text)['predictions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can show the prediction result locally on the Notebook, using `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show(idx, title):\n",
    "    plt.figure()\n",
    "    plt.imshow(test_images[idx].reshape(28,28))\n",
    "    plt.axis('off')\n",
    "    plt.title('\\n\\n{}'.format(title), fontdict={'size': 16})\n",
    "\n",
    "show(0, 'The model thought this was a {} (class {}), and it was actually a {} (class {})'.format(\n",
    "  class_names[np.argmax(predictions[0])], np.argmax(predictions[0]), class_names[test_labels[0]], test_labels[0]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Jupyter Workflow",
   "language": "python",
   "name": "jupyter-workflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "workflow": {
   "models": {
    "tensorflow": {
     "config": {
      "file": "environment/hpc/ssh_template.jinja2",
      "hostname": "hostname",
      "sshKey": "/path/to/ssh/key",
      "username": "username"
     },
     "type": "ssh"
    },
    "tf-serving": {
     "config": {
      "chart": "environment/k8s/tensorflow-serving",
      "releaseName": "tf-serving"
     },
     "type": "helm"
    }
   },
   "serializers": {
    "model": {
     "postload": "from tensorflow.python.platform import tf_logging\nfrom tensorflow.keras.models import load_model\nverbosity = tf_logging.get_verbosity()\ntf_logging.set_verbosity(tf_logging.ERROR)\nmodel = load_model(model)\ntf_logging.set_verbosity(verbosity)",
     "predump": "from tensorflow.python.platform import tf_logging\nimport tempfile\ndirpath = tempfile.mkdtemp()\nmodel.save(dirpath)\nverbosity = tf_logging.get_verbosity()\ntf_logging.set_verbosity(tf_logging.ERROR)\nmodel = dirpath\ntf_logging.set_verbosity(verbosity)",
     "type": "file"
    }
   },
   "version": "v1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
